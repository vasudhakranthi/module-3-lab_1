{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudhakranthi/module-3-lab_1/blob/main/Mod3_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad**\n",
        "### MODULE 3: CLASSIFICATION-1\n",
        "### LAB-2 : Implementing KNN from scratch and visualize Algorithm performance\n",
        "#### Module Coordinator: Jashn Arora\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7lNmnA0_AhlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Section 1: Implementing KNN**"
      ],
      "metadata": {
        "id": "Q7v6N5-LBHWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last lab we had started discussing about KNN or K Nearest Neighbour method for clasification. We used the pre-built scikit-learn library for KNN. Now let's see how to implement this algorithm from scratch  "
      ],
      "metadata": {
        "id": "DIKaq_foFSXD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7877e44n6Kd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHzdQE_8oPM1"
      },
      "source": [
        "def predict(X_train, y_train, X_test, k):\n",
        "    distances = []\n",
        "    targets = []\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        # compute and store L2 distance\n",
        "        distances.append([np.sqrt(np.sum(np.square(X_test - X_train[i, :]))), i])\n",
        "\n",
        "    distances = sorted(distances)\n",
        "\n",
        "    for i in range(k):\n",
        "        index = distances[i][1]\n",
        "        targets.append(y_train[index])\n",
        "\n",
        "    # return most common target\n",
        "    return Counter(targets).most_common(1)[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_kulEM0oWJ-"
      },
      "source": [
        "def k_nearest_neighbor(X_train, y_train, X_test, k):\n",
        "\n",
        "    assert k <= len(X_train), \"[!] K cannot be larger than number of samples.\"\n",
        "\n",
        "    # loop over all observations\n",
        "    predictions = []\n",
        "    for i in range(len(X_test)):\n",
        "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
        "\n",
        "    return np.asarray(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** : If k = 1 then the algorithm will simply return the label of the nearest neighbour. When we give k > 1 the most common label out of the given labels in the k neighbours will be selected.The code for 1 NN is given as follows and does not have to be so complicated."
      ],
      "metadata": {
        "id": "W_saK1gP2zd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label"
      ],
      "metadata": {
        "id": "wa4lAr9p3Tqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1.1 - Iris Dataset\n",
        "Let's try it out on Iris Dataset present in the scikit learn library"
      ],
      "metadata": {
        "id": "97MTzRbMKHfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
        "\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VJ7YJCT1KN29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(df[['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']])\n",
        "y = np.array(df['target'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=17)"
      ],
      "metadata": {
        "id": "r8SHprUbKtuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making our predictions\n",
        "predictions = k_nearest_neighbor(X_train, y_train, X_test, 7)\n",
        "\n",
        "# evaluating accuracy\n",
        "accuracy = Accuracy(y_test, predictions)\n",
        "print(\"The accuracy of our classifier is {} %\".format(100*accuracy))"
      ],
      "metadata": {
        "id": "SUYb80o6LHUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwQ3qP-UXY5x"
      },
      "source": [
        "Voila! You have implemented your own version of the K-Nearest Neighbours algorithm, which works very well on the Iris Dataset. Congratulations!  \n",
        "\n",
        "Now try out the sklearn implementation and compare your results.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0jJj5_7dg-o"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "sklearn_knn = KNeighborsClassifier(n_neighbors=7)\n",
        "sklearn_knn.fit(X_train,y_train)\n",
        "sklearn_predictions = sklearn_knn.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, sklearn_predictions)\n",
        "print(\"The accuracy of Sklearn classifier is {} %\".format(100*accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Section 1.2: Weighted NN\n",
        "For KNN, If k is too small, the algorithm would be more sensitive to outliers. If k is too large, then the neighborhood may include too many points from other classes. We also take the label with max votes in the neighbourhood. Another choice can be that instead of giving equal weights to each of the neighbours we can give weighted points in the neighbourhood. So we weigh points by the inverse of their distance. Therefore, closer points will be given a higher priority as compared to the far off points.\n",
        "\n",
        "An easy way to implement this is by specifying the 'weights' parameter as distance when defining the sklearn KNN function. For more information go through this [site](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
      ],
      "metadata": {
        "id": "8Yi2tX6h0_Ka"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hXZP4wXqtmK"
      },
      "source": [
        "## TASK\n",
        "## Modify the KNN function you wrote to return all the K-nearest neighbours along with their distances,\n",
        "## instead of just the output that was most common. You don't need to find out accuracy, just modify the function\n",
        "## and return the k-nearest neighbours and distances."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxcBnLl8GHWz"
      },
      "source": [
        "# **Section 2: Visualizing Data**  \n",
        "\n",
        "We will look into something called **Voronoi** diagrams.  \n",
        "\n",
        "**Note**: Ideally, we should perform data visualization to see what the data looks like before we apply any Machine Learning algorithm.  Only for the purpose of this lab session, we're explaining it after you've applied KNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aDsDfmXG79k"
      },
      "source": [
        "### Voronoi Diagrams  \n",
        "\n",
        "In simple terms, Voronoi diagrams help you to visualize the dataset by partioning the plane into regions that are close to a given set of points. These regions are also called Voronoi cells.  \n",
        "\n",
        "Note that the cells/regions depend on the Distance metric being used. One way of interpreting this is by understanding that the distance metric decides the degree to which a 'point' or 'seed' in the Voronoi diagram has influence.  For each seed there is a corresponding region, called a Voronoi cell, consisting of all points of the plane closer to that seed than to any other.\n",
        "\n",
        "This [link](https://en.wikipedia.org/wiki/Voronoi_diagram#Illustration) provides a wonderful illustration of Voronoi plots for 20 points in two cases: (1) Using Euclidean distance, and (2) Using Manhattan distance.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdcdjthOKwra"
      },
      "source": [
        "Since our Iris data is 4-dimensional (as it has 4 attributes), we need to convert into a form that can be represented in 2-D.   \n",
        "\n",
        "While there are methods to visualize data higher than 2-dimensions, that is beyond scope for now.  \n",
        "\n",
        "For simplicity, we just take the first two columns of the iris dataset attributes and observe the Voronoi diagram generated for that.  \n",
        "Alternatively, one can also perform PCA (Principal Component Analysis), to reduce the 4D data to just two dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNOPcS3f7sZg",
        "cellView": "form"
      },
      "source": [
        "#@title Plotting Voronoi regions\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "\n",
        "def voronoi_finite_polygons_2d(vor, radius=None):\n",
        "    \"\"\"\n",
        "    Reconstruct infinite voronoi regions in a 2D diagram to finite\n",
        "    regions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    vor : Voronoi\n",
        "        Input diagram\n",
        "    radius : float, optional\n",
        "        Distance to 'points at infinity'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    regions : list of tuples\n",
        "        Indices of vertices in each revised Voronoi regions.\n",
        "    vertices : list of tuples\n",
        "        Coordinates for revised Voronoi vertices. Same as coordinates\n",
        "        of input vertices, with 'points at infinity' appended to the\n",
        "        end.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if vor.points.shape[1] != 2:\n",
        "        raise ValueError(\"Requires 2D input\")\n",
        "\n",
        "    new_regions = []\n",
        "    new_vertices = vor.vertices.tolist()\n",
        "\n",
        "    center = vor.points.mean(axis=0)\n",
        "    if radius is None:\n",
        "        radius = vor.points.ptp().max()\n",
        "\n",
        "    # Construct a map containing all ridges for a given point\n",
        "    all_ridges = {}\n",
        "    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
        "        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
        "        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
        "\n",
        "    # Reconstruct infinite regions\n",
        "    for p1, region in enumerate(vor.point_region):\n",
        "        vertices = vor.regions[region]\n",
        "\n",
        "        if all(v >= 0 for v in vertices):\n",
        "            # finite region\n",
        "            new_regions.append(vertices)\n",
        "            continue\n",
        "\n",
        "        # reconstruct a non-finite region\n",
        "        ridges = all_ridges[p1]\n",
        "        new_region = [v for v in vertices if v >= 0]\n",
        "\n",
        "        for p2, v1, v2 in ridges:\n",
        "            if v2 < 0:\n",
        "                v1, v2 = v2, v1\n",
        "            if v1 >= 0:\n",
        "                # finite ridge: already in the region\n",
        "                continue\n",
        "\n",
        "            # Compute the missing endpoint of an infinite ridge\n",
        "\n",
        "            t = vor.points[p2] - vor.points[p1] # tangent\n",
        "            t /= np.linalg.norm(t)\n",
        "            n = np.array([-t[1], t[0]])  # normal\n",
        "\n",
        "            midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
        "            direction = np.sign(np.dot(midpoint - center, n)) * n\n",
        "            far_point = vor.vertices[v2] + direction * radius\n",
        "\n",
        "            new_region.append(len(new_vertices))\n",
        "            new_vertices.append(far_point.tolist())\n",
        "\n",
        "        # sort region counterclockwise\n",
        "        vs = np.asarray([new_vertices[v] for v in new_region])\n",
        "        c = vs.mean(axis=0)\n",
        "        angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n",
        "        new_region = np.array(new_region)[np.argsort(angles)]\n",
        "\n",
        "        # finish\n",
        "        new_regions.append(new_region.tolist())\n",
        "\n",
        "    return new_regions, np.asarray(new_vertices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NIVhxz8KvG3"
      },
      "source": [
        "## Plotting Voronoi regions for the Iris dataset\n",
        "\n",
        "points = []\n",
        "xpts = np.array(df['sepal length (cm)'])\n",
        "ypts = np.array(df['sepal width (cm)'])\n",
        "for i in range(len(xpts)):\n",
        "  points.append([xpts[i],ypts[i]])\n",
        "# print(points)\n",
        "points = np.array(points)\n",
        "# compute Voronoi tesselation\n",
        "vor = Voronoi(points)\n",
        "\n",
        "regions, vertices = voronoi_finite_polygons_2d(vor)\n",
        "\n",
        "for region in regions:\n",
        "    polygon = vertices[region]\n",
        "    plt.fill(*zip(*polygon), alpha=0.4)\n",
        "\n",
        "plt.plot(points[:,0], points[:,1], 'ko')\n",
        "plt.xlim(vor.min_bound[0] - 0.1, vor.max_bound[0] + 0.1)\n",
        "plt.ylim(vor.min_bound[1] - 0.1, vor.max_bound[1] + 0.1)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSK-GI0Knurk"
      },
      "source": [
        "## Section 2.2: Understanding Decision Boundaries  \n",
        "So you have seen the Voronoi diagram of the dataset, implemented KNN, and also seen your algorithm's performance in terms of accuracy? Impressive!  \n",
        "Wouldn't it also be great to know how exactly these 'votes' or neighbours are decided through some kind of visualization?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytP_AImALiZX"
      },
      "source": [
        "### Decision Boundary\n",
        "\n",
        "While the Voronoi diagram gave us a good idea of the points present in our dataset, to understand how KNN performed on our dataset we can plot decision boundaries. Decision boundaries, as the name suggests, divide the plane into different regions of classification.  \n",
        "\n",
        "Note that here again, for simplicity, we have only considered first two attributes of the DataFrame (ie, Sepal Length and Sepal Width).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P8Pqav4DI4N"
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def decision_boundary_plot(x_dec,y_dec,k):\n",
        "  h = .02  # step size in the mesh\n",
        "\n",
        "  # Create color maps\n",
        "  n = len(set(y_dec))\n",
        "  cmap_light = ListedColormap(['pink', 'green', 'cyan','yellow'][:n])\n",
        "  cmap_bold = ['pink', 'darkgreen', 'blue','yellow'][:n]\n",
        "\n",
        "  for weights in ['uniform', 'distance']:\n",
        "      # we create an instance of Neighbours Classifier and fit the data.\n",
        "      clf = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
        "      clf.fit(x_dec, y_dec)\n",
        "\n",
        "      # Plot the decision boundary. For that, we will assign a color to each\n",
        "      # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "      x_min, x_max = x_dec[:, 0].min() - 1, x_dec[:, 0].max() + 1\n",
        "      y_min, y_max = x_dec[:, 1].min() - 1, x_dec[:, 1].max() + 1\n",
        "      xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                          np.arange(y_min, y_max, h))\n",
        "      Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "      # Put the result into a color plot\n",
        "      Z = Z.reshape(xx.shape)\n",
        "      plt.figure(figsize=(8, 6))\n",
        "      plt.contourf(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "      # Plot also the training points\n",
        "      sns.scatterplot(x=x_dec[:, 0], y=x_dec[:, 1], hue=y_dec,\n",
        "                      palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n",
        "      plt.xlim(xx.min(), xx.max())\n",
        "      plt.ylim(yy.min(), yy.max())\n",
        "      plt.title(\"Multi-Classification (k = %i, weights = '%s')\"% (k, weights))\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgAO62vWKPHt"
      },
      "source": [
        "x_pts = X[:,:2]\n",
        "y_pts = y\n",
        "decision_boundary_plot(x_pts,y_pts,7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFRJidIgr6gt"
      },
      "source": [
        "## TASK-2\n",
        "## In the above cells, we saw the Voronoi diagram of the data and plotted the KNN decision boundaries\n",
        "## by only considering two attributes of the dataset. You must be already familiar with PCA.\n",
        "## Apply PCA on the dataset above to reduce it to two dimensions.\n",
        "## Plot the Voronoi diagram and Decision boundaries after that."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gti-Jgu_VBDl"
      },
      "source": [
        "## Section 2.3: Confusion Matrix  \n",
        "In classification problems, a confusion matrix, also known as an error matrix, is a table that allows visualization of the performance of an algorithm, typically a supervised learning one. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPfc8YFBA8Oh"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMXIM9apA99s"
      },
      "source": [
        "# print(confusion_matrix(y_test,predictions))\n",
        "pd.crosstab(y_test, predictions, rownames=['True'], colnames=['Predicted'], margins=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cgYG0E5UHdy"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, predictions)\n",
        "p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"viridis\" ,fmt='g')\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTPQOPStVtmI"
      },
      "source": [
        "## Section 2.4: Classification Report\n",
        "\n",
        "Precision, Recall, and F1-Score are other metrics besides accuracy that one might look for in an algorithm.  Depending on the use-case, one might consider one metric more important than the other.  \n",
        "\n",
        "Note: *T-> True, F->False, P->Positive, N->Negative*\n",
        "    \n",
        "Mathematically, Accuracy is :  \n",
        "\n",
        "$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$  \n",
        "\n",
        "**Precision**: The accuracy of positive predictions\n",
        "\n",
        "\n",
        "$Precision = \\frac{TP}{TP+FP}$\n",
        "\n",
        "**Recall**:Fraction of positives that were correctly identified\n",
        "\n",
        "\n",
        "$Recall = \\frac{TP}{TP+FN}$\n",
        "\n",
        "\n",
        "**F1-score**: Harmonic mean of precision and recall  \n",
        "\n",
        "\n",
        "$F1 = \\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN}$  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH3KEfEYW190"
      },
      "source": [
        "#import classification_report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtYNFoJh0LU-"
      },
      "source": [
        "### **Car Evaluation Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsBukCMi4UjJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "fdedf45b-5de0-480e-edd7-885f8894f585"
      },
      "source": [
        "# Upload the Car evaluation data CSV file that has been shared with you.\n",
        "# Run this cell, click on the 'Choose files' button and upload the file.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e729b63a-7d85-4de9-b6ae-914783c44cb2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e729b63a-7d85-4de9-b6ae-914783c44cb2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T5yvzVH4jrQ"
      },
      "source": [
        "car_df = pd.read_csv('car_evaluation.csv')\n",
        "car_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwjBankjB9Os"
      },
      "source": [
        "for x in car_df.columns:\n",
        "  # print(x)\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  le.fit(car_df[x])\n",
        "  car_df[x]=le.transform(car_df[x])\n",
        "\n",
        "car_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5Klx4yMCgKa"
      },
      "source": [
        "dataset = car_df.values\n",
        "X = dataset[:,0:6]\n",
        "y = np.array(dataset[:,6])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=19)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HL5ufCHDANh"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "predictions = knn.predict(X_test)\n",
        "score = accuracy_score(y_test, predictions)\n",
        "print(\"The accuracy of the classifier on Car evaluation dataset is {:.2f} %\".format(100*score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYCimUC_A56C"
      },
      "source": [
        "## TASK-3\n",
        "## Plot a Confusion Matrix for the results of the Car evaluation dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8TRknp0XTqJ"
      },
      "source": [
        "## TASK-4\n",
        "## Print a Classification Report for the results of the Car evaluation dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK-5\n",
        "## Plot the Decision boundary diagram for the classifier of the Car evaluation dataset"
      ],
      "metadata": {
        "id": "yJFiD9dzNt-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK-6\n",
        "## Plot the Voronoi diagram for the classifier of the Car evaluation dataset"
      ],
      "metadata": {
        "id": "CUVQ-oD6OA8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr9dI1Kqsprz"
      },
      "source": [
        "### Questions to Think About and Answer\n",
        "1. In the section on Decision boundaries, you must have seen that we ran the KNN algorithm twice: first with the _weights_ set to 'uniform' and then set to 'distance'. Find out the difference between these two.  \n",
        "The difference between uniform and distance weights in the k-Nearest Neighbors (k-NN) algorithm lies in how the neighbors are weighted when making predictions. This affects how the influence of each neighbor is calculated when determining the output for a query point.\n",
        "\n",
        "1. Uniform Weights\n",
        "Description:\n",
        "All neighbors contribute equally to the prediction, regardless of their distance from the query point.\n",
        "The weight assigned to each neighbor is 1.\n",
        "Effect:\n",
        "The prediction is essentially the average (for regression) or the majority class (for classification) among the\n",
        "𝑘\n",
        "k-nearest neighbors.\n",
        "Use Case:\n",
        "Useful when all neighbors are considered equally reliable, such as in datasets where distances do not strongly correlate with relevance.\n",
        "2. Distance Weights\n",
        "Description:\n",
        "Closer neighbors are given higher weight, and farther neighbors are given lower weight.\n",
        "The weight is inversely proportional to the distance (e.g.,\n",
        "weight\n",
        "=\n",
        "1\n",
        "distance\n",
        "weight=\n",
        "distance\n",
        "1\n",
        "​\n",
        " ).\n",
        "Effect:\n",
        "Predictions are influenced more heavily by neighbors that are closer to the query point.\n",
        "Helps mitigate the influence of distant neighbors, which might be less relevant.\n",
        "Use Case:\n",
        "Useful in datasets where closer neighbors are more likely to provide better predictions (e.g., spatial or geographic data, or when the feature space has strong locality).\n",
        "Comparison of the Two\n",
        "Aspect\tUniform Weights\tDistance Weights\n",
        "Weight Calculation\tAll neighbors have equal weight.\tCloser neighbors have higher weight.\n",
        "Sensitivity to Distance\tIgnores distance information.\tConsiders distance information.\n",
        "Influence\tAll\n",
        "𝑘\n",
        "k-neighbors are equally important.\tDistant neighbors contribute less.\n",
        "Best for\tData with uniformly distributed points or no strong locality.\tData where proximity is a significant factor.\n",
        "Impact on Decision Boundaries\n",
        "With uniform weights, decision boundaries are more influenced by the density and spread of neighbors, as all are treated equally.\n",
        "With distance weights, decision boundaries can shift to reflect closer neighbors' dominance, potentially resulting in smoother or more localized boundaries.\n",
        "In practice, choosing between uniform and distance weights depends on the dataset and the problem at hand. Distance weights often perform better when closer neighbors are more representative of the target.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. What do you think could be the drawbacks of using KNN ?\n",
        "1. Computational Cost\n",
        "Reason: k-NN is a lazy learner, meaning it does not build a model in advance but stores all training data and computes distances at prediction time.\n",
        "Impact:\n",
        "Prediction requires calculating the distance to all training points, which can be computationally expensive, especially for large datasets or high-dimensional data.\n",
        "The cost of searching for the nearest neighbors increases with dataset size.\n",
        "Solution: Use approximate nearest neighbor algorithms (e.g., KD-trees, Ball-trees) or dimensionality reduction techniques.\n",
        "2. Sensitivity to the Choice of\n",
        "𝑘\n",
        "k\n",
        "Reason: The choice of\n",
        "𝑘\n",
        "k can significantly affect performance:\n",
        "A small\n",
        "𝑘\n",
        "k: Prone to overfitting and noise sensitivity.\n",
        "A large\n",
        "𝑘\n",
        "k: Can lead to underfitting, as distant neighbors may dominate.\n",
        "Impact: Poor performance if\n",
        "𝑘\n",
        "k is not tuned carefully.\n",
        "Solution: Use techniques like cross-validation to find the optimal\n",
        "𝑘\n",
        "k.\n",
        "3. Curse of Dimensionality\n",
        "Reason: In high-dimensional spaces, all points tend to become equidistant (distance measures lose their discriminative power).\n",
        "Impact:\n",
        "Degraded performance as the algorithm struggles to find meaningful neighbors.\n",
        "Increased computational cost due to the high dimensionality.\n",
        "Solution: Dimensionality reduction techniques (e.g., PCA, t-SNE) or feature selection can help.\n",
        "4. Sensitivity to Irrelevant and Redundant Features\n",
        "Reason: k-NN uses all features to compute distances, which can dilute the impact of relevant features.\n",
        "Impact:\n",
        "Poor accuracy if irrelevant or redundant features dominate the distance metric.\n",
        "Solution: Perform feature selection or weighting to prioritize important features.\n",
        "5. Imbalanced Data Issues\n",
        "Reason: k-NN assigns labels based purely on the majority of neighbors, which can bias it towards the majority class in imbalanced datasets.\n",
        "Impact:\n",
        "Poor performance on the minority class.\n",
        "Solution: Use techniques like oversampling, undersampling, or assigning weights to the classes.\n",
        "6. Lack of Interpretability\n",
        "Reason: k-NN does not provide explicit models or insights into the relationships between features and the target.\n",
        "Impact:\n",
        "Difficult to explain predictions.\n",
        "Solution: Use interpretable models (e.g., decision trees) or model explanations alongside k-NN.\n",
        "7. Memory Intensive\n",
        "Reason: k-NN requires storing the entire dataset.\n",
        "Impact:\n",
        "High memory requirements, especially for large datasets.\n",
        "Solution: Consider alternative algorithms like support vector machines (SVM) or decision trees.\n",
        "Summary\n",
        "While k-NN is simple and effective in certain cases, its drawbacks include high computational costs, sensitivity to the choice of\n",
        "𝑘\n",
        "k, challenges in high-dimensional spaces, and issues with interpretability and scalability. Addressing these limitations often requires preprocessing, hyperparameter tuning, or switching to more sophisticated algorithms.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRlrn4EctZrC"
      },
      "source": [
        "### Useful Resources for further reading\n",
        "1. Interactive KNN visualization, with class boundaries: http://vision.stanford.edu/teaching/cs231n-demos/knn/  \n"
      ]
    }
  ]
}